def category_raw(args):
    """
    function for identifying which high-level categories are present in a text, and 
    High-level categories are taken from the THINGS+ dataset (generated by WordNet).
    """
    import os
    import spacy
    import tqdm
    import pandas as pd
    from collections import Counter

    # The filepath of the categories file (should never change).
    THINGS_dir = os.path.join(args.project_dir, 'eeg_dataset', 'wake_data', 'THINGS')
    import_path_categories = os.path.join(THINGS_dir, "category53_longFormat.tsv")

    # The filepath of the dream reports file.
    Zhang_dir = os.path.join(args.project_dir, 'eeg_dataset', 'dream_data', 'Zhang_Wamsley')
    import_path_reports = os.path.join(Zhang_dir, 'Data', 'Reports.csv')

    # The column name of the dream reports (should be present in the dream reports file).
    reports_column = "Text of Report"

    # Load both files.
    df_categories = pd.read_csv(import_path_categories, sep="\t")
    df_reports = pd.read_csv(import_path_reports)

    # Extract a dictionary that maps words to categories.
    word2category_mapping = df_categories.set_index("Word")["category"].to_dict()

    # Extract the dream reports as a list.
    reports = df_reports[reports_column].tolist()

    # Load the spaCy model.
    nlp = spacy.load("en_core_web_sm")

    # Create an empty list to hold the results.
    results = []
    # Loop over each dream report.
    for report in tqdm.tqdm(reports):
        # Tokenize and lemmatize the dream report (i.e., split into words and get root word form).
        doc = nlp(report)
        lemmas = [token.lemma_ for token in doc]
        # Get word categories for each word that belongs to one.
        lemma_categories = [word2category_mapping.get(lemma, "none") for lemma in lemmas]
        # Get counts of each word category.
        category_counts = Counter(lemma_categories)
        # Append this result to the results list.
        results.append(category_counts)

    # Convert the results to a DataFrame.
    df = pd.Series(results).apply(pd.Series).fillna(0).astype(int)

    # The list of the indices of dream reports under the same dream category
    report_idx = []

    for index, row in df.iterrows():
        if row[args.category] != 0:
            report_idx.append(index)

    # The list of 'subject ID' under the same dream category
    IDs = df_reports.loc[report_idx, 'Case ID'].tolist()
    IDs = [f"{'0' + num if len(num) == 2 else num}_{text}" for num, text in [ID.split('_') for ID in IDs]]
    print(f'The number of dreams under category {args.category}: {len(IDs)}')
    
    return IDs

def epoching(args, raw):
    import os
    import mne
    import numpy as np

    ### Load the sample THINGS data ###
    # Load the sample THINGS prepr test dir 
    THINGS_prepr_dir = os.path.join(args.project_dir, 'eeg_dataset', 'wake_data', 'THINGS',
    'preprocessed_data')
    # Load sample THINGS prepr test data 
    THINGS_sample_data = np.load(os.path.join(THINGS_prepr_dir, 'occipital', 'sub-01', 
                                              'preprocessed_eeg_test.npy'), allow_pickle=True).item()
    # Load THINGS channels 
    THINGS_ch = THINGS_sample_data['ch_names']
    del THINGS_sample_data
    # Capitalize THINGS channel names
    THINGS_ch = [ch.upper() for ch in THINGS_ch]

    ### Preprocess the dream data ###
    chan_idx = np.asarray(mne.pick_channels_regexp(raw.info['ch_names'],
        '^O *|^P *'))
    new_chans = [raw.info['ch_names'][c] for c in chan_idx]
    raw.pick(new_chans)
    # Resampling 
    raw = raw.resample(args.sfreq)
    # Get channel names
    dream_ch = raw.info['ch_names']
    # Capitalize and shorten dream channel names
    dream_ch = [ch[:-4].upper() for ch in dream_ch]
    # Extract the index of the channel in THINGS EEG which presents in THINGS EEG but not in dream
    extra_dream_ch_idx = [i for i, ch in enumerate(dream_ch) if ch not in THINGS_ch]
    # Get times
    times = raw.times
    # Get epoched data
    epoched_data = raw.get_data()  
    # Creat sub epoched data by removing extra dream EEG channels 
    if len(extra_dream_ch_idx) != 0:
        sub_epoched_data = np.delete(epoched_data, extra_dream_ch_idx, axis=0)
    else:
        sub_epoched_data = epoched_data
    del epoched_data

    ### Sort the dream EEG data ###
    # Extract the index of the channel in THINGS EEG which presents in THINGS EEG but not in dream
    extra_THINGS_ch_idx = next(i for i, ch in enumerate(THINGS_ch) if ch not in dream_ch)
    print(f'The redundant EEG channel idx in THINGS: {extra_THINGS_ch_idx}')
    # Drop the redundant channel from THINGS channels 
    THINGS_ch.pop(extra_THINGS_ch_idx) 
    # Sort the dream channel indices
    dream_idx = [dream_ch.index(ch) for ch in THINGS_ch]
    # Sort dream EEG data according to the dream channel indices
    sorted_epoched_data = np.empty((sub_epoched_data.shape))
    # Sort dream EEG channels according to the dream channel indices
    ch_names = []
    for ii, i in enumerate(dream_idx):
        sorted_epoched_data[ii] = sub_epoched_data[i] 
        ch_names.append(dream_ch[i])
    del sub_epoched_data
    
    ### Output ###
    return sorted_epoched_data, ch_names, times

def mvnn(args, epoched_data):
    import numpy as np
    from sklearn.discriminant_analysis import _cov
    import scipy
    
    ### Compute the covariance matrices with shape: EEG channels Ã— EEG channels  ###
    sigma = np.empty((epoched_data.shape[0],  epoched_data.shape[0]))
    
    # Compute covariace matrices at each time point, and then
    # average across time points
    if args.mvnn_dim == "time":
        sigma = _cov(epoched_data.T, shrinkage='auto')

    # Compute the inverse of the covariance matrix
    sigma_inv = scipy.linalg.fractional_matrix_power(sigma, -0.5) 
    # The matrix power is -0.5, which represents inverse square root

    ### Whiten the data ###
    whitened_data = np.reshape((epoched_data.swapaxes(0, 1) @ sigma_inv).swapaxes(0, 1), 
                               (epoched_data.shape[0], epoched_data.shape[1]))
    ### Output ###
    return whitened_data

def save_prepr(args, whitened_data, ch_names, times, ID_select):
    import numpy as np
    import os

    ### Save the data ###
    save_dir = os.path.join(args.project_dir, 'eeg_dataset', 'dream_data', 'Zhang_Wamsley', 'preprocessed_data')
    # Create the directory if it doesn't exist #
    if not os.path.isdir(save_dir):
        os.makedirs(save_dir)

    # Insert the data into a dictionary #
    my_dict = {
        'preprocessed_eeg_data': whitened_data,
        'ch_names': ch_names,
        'times': times
    }

    folder_name = args.category
    save_folder = os.path.join(save_dir, folder_name)
    if not os.path.isdir(save_folder):
        os.makedirs(save_folder)
    file_name = 'prepr_'+ID_select
    np.save(os.path.join(save_folder, file_name), my_dict)
    del my_dict